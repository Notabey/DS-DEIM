__include__: [
  '../dataset/coco_detection.yml',
  '../runtime.yml',
  '../base/dataloader.yml',
  '../base/optimizer.yml',
  '../base/deimv2.yml',
]

# Increase to search for the optimal ema
epoches: 132 # 120 + 4n

## Our LR-Scheduler
flat_epoch: 64    # 4 + epoch // 2, e.g., 40 = 4 + 72 / 2
no_aug_epoch: 12

output_dir: ./outputs/dsdeim_s_coco

DEIM:
  backbone: DSViT

DSViT:
  embed_dims: [128, 192]
  depths: [2, 12]  # Stage 2 matches ViT-Tiny depth (12 layers)
  num_heads: 3    # Match ViT-Tiny (3 heads, 64 dim) for weight loading compatibility
  mlp_ratio: 4.   # Match ViT-Tiny (Expansion 4) for weight loading compatibility
  pretrained: ./ckpts/vitt_distill.pt
  drop_path_rate: 0.0     # Reverted to 0.0 as requested

HybridEncoder:
  in_channels: [128, 192, 192]  # P3=128(stage1), P4=192(stage2), P5=192(stage2_down) 
  depth_mult: 0.67
  expansion: 0.34
  hidden_dim: 192
  dim_feedforward: 512
  # Logic: Student (192) projects TO Teacher (768). 
  # This param creates: Linear(192, 768).
  distill_teacher_dim: 768 
  
  # AIFI Configuration
  num_encoder_layers: 0  # Disable AIFI (no transformer encoder)
  use_encoder_idx: [1]   # Target P4 (idx=1) for distillation

DEIMTransformer:
  feat_channels: [192, 192, 192]
  hidden_dim: 192
  dim_feedforward: 512
  num_layers: 4  # 4 5 6
  eval_idx: -1  # -2 -3 -4


DEIMCriterion:
  # Aligned with RT-DETRv4
  weight_dict: {loss_mal: 1, loss_bbox: 5, loss_giou: 2, loss_fgl: 0.15, loss_ddf: 1.5, loss_distill: 5.0}
  losses: ['mal', 'boxes', 'local', 'distill']
  
  distill_adaptive_params:
    enabled: True
    rho: 11
    delta: 1
    default_weight: 20.0
  
  distill_temp_schedule:
    enabled: True
    start_temp: 0.04
    end_temp: 0.1
    warmup_epochs: 20
  
  matcher:
    type: HungarianMatcher
    weight_dict: {cost_class: 2, cost_bbox: 5, cost_giou: 2}
    alpha: 0.25
    gamma: 2.0
    change_matcher: True
    iou_order_alpha: 4.0
    matcher_change_epoch: 100
  
  # Distill P4 (index 1) from Teacher [P3, P4, P5]
  teacher_out_idx: 1

optimizer:
  type: AdamW
  params: 
    -
      # Group 1: Random Init Backbone Layers (Stem, Stage1, Transition, P3_fusion)
      # Use higher LR to learn features from scratch
      params: '^(?=.*backbone)(?=.*(?:stem|stage1|transition|p3_fusion))(?!.*(?:norm|bn)).*$'
      lr: 0.0004
    -
      # Group 2: Pretrained Backbone Layers (Stage2)
      # Use lower LR to preserve pretrained ViT-Tiny weights
      params: '^(?=.*backbone)(?=.*stage2)(?!.*(?:norm|bn)).*$'
      lr: 0.00005
    -
      # Other params (Neck, Head) - implicitly handled by default lr if not matched? 
      # No, we need to be careful. The next group matches 'norm|bn'.
      # Any param NOT matched by these regexes uses the global 'lr: 0.0004'.
      # So simple backbone regex splitting is enough.
      
      # Group 3: All norm and bn layers (no weight decay)
      params: '^(?=.*(?:norm|bn)).*$'
      weight_decay: 0.

  lr: 0.0004
  betas: [0.9, 0.999]
  weight_decay: 0.0001

train_dataloader:
  total_batch_size: 32 # Adjusted per user
  dataset:
    transforms:
      ops:
        - {type: Mosaic, output_size: 320, rotation_range: 10, translation_range: [0.1, 0.1], scaling_range: [0.5, 1.5],
           probability: 1.0, fill_value: 0, use_cache: True, max_cached_images: 50, random_pop: True}
        - {type: RandomPhotometricDistort, p: 0.5}
        - {type: RandomZoomOut, fill: 0}
        - {type: RandomIoUCrop, p: 0.8}
        - {type: SanitizeBoundingBoxes, min_size: 1}
        - {type: RandomHorizontalFlip}
        - {type: Resize, size: [640, 640], }
        - {type: SanitizeBoundingBoxes, min_size: 1}
        - {type: ConvertPILImage, dtype: 'float32', scale: True}
        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}
        - {type: ConvertBoxes, fmt: 'cxcywh', normalize: True}
      # Mosaic options
      policy:
        epoch: [4, 64, 120]   # list 
        ops: ['Mosaic', 'RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']

  collate_fn:
    base_size: 640
    mixup_prob: 0.5
    ema_restart_decay: 0.9999
    base_size_repeat: 20
    mixup_epochs: [4, 64]
    stop_epoch: 120
    copyblend_epochs: [4, 120]

val_dataloader:
  dataset:
    transforms:
      ops:
        - {type: Resize, size: [640, 640], }
        - {type: ConvertPILImage, dtype: 'float32', scale: True}
        - {type: Normalize, mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]}

# Minimal Teacher Config for DS-DEIM
teacher_model:
  type: DINOv3TeacherModel
  dinov3_weights_path: /media/wxy/TiPro9000/weights/DINOv3_vitb16 
  dinov3_model_type: dinov3_vitb16
  patch_size: 16

find_unused_parameters: False